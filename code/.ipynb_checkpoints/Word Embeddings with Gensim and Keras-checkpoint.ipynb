{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Word2Vec is one algorithm for learning a word embedding from a text corpus. There are two main training algorithms that can be used to learn the embedding from text; they are Continuous Bag-of-Words (CBOW) and skip grams.\n",
    "\n",
    "Gensim provides the Word2Vec class for working with a Word2Vec model. Learning a word embedding from text involves loading and organizing the text into sentences and providing them to the constructor of a new Word2Vec() instance.\n",
    "\n",
    "Word2Vec sample parameters:\n",
    "\n",
    "    - size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "    - window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "    - min count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "    - workers: (default 3) The number of threads to use while training.\n",
    "    - sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "    \n",
    "After the model is trained, it is accessible via the wv attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "vector for 'sentence':  [ 3.1439364e-03  9.5028500e-04  2.7835707e-03  3.1352739e-03\n",
      "  4.2390935e-03 -1.5161904e-03  1.1548786e-03 -4.2492128e-03\n",
      "  1.9076599e-03 -3.2465404e-03 -4.9408539e-03  2.3700439e-03\n",
      " -4.2715500e-04  4.0828786e-03 -3.4725328e-03  1.9437830e-03\n",
      "  4.2161522e-03  2.9343886e-03  2.9821701e-03 -4.0986361e-03\n",
      "  4.2177397e-03 -2.0715145e-03 -4.8050778e-03 -1.5136076e-03\n",
      " -4.5195497e-03  6.6057814e-04  3.9527742e-03  1.4317381e-03\n",
      "  1.0629682e-04  2.5963661e-06 -2.2794516e-03  2.5774001e-03\n",
      "  8.8775752e-04 -1.5389838e-03  1.7023112e-03 -3.9838531e-04\n",
      "  4.5883250e-03 -6.8975094e-04  1.7323573e-03 -3.4168628e-03\n",
      " -3.0757834e-03  7.8514713e-04 -2.8028227e-03  1.9703605e-03\n",
      " -2.1698221e-03  4.6723234e-03 -5.1325891e-04 -5.4181216e-04\n",
      "  4.8918915e-03 -4.1848770e-03 -4.9417648e-03  3.6112056e-03\n",
      "  3.8742956e-03 -3.8131818e-03  1.1591470e-03  1.1874930e-03\n",
      "  7.0525770e-04 -2.1823398e-03 -1.2447084e-03  4.4521741e-03\n",
      "  4.4216090e-03 -2.8216010e-03  3.4833269e-03 -1.6725005e-03\n",
      "  4.3722535e-03 -2.2384841e-03 -4.9577467e-03 -7.5870560e-04\n",
      "  1.5657922e-03  1.2605665e-03  4.6630097e-03  4.0137167e-03\n",
      "  1.4339248e-03 -1.2732393e-03 -3.6917808e-03  1.1613521e-03\n",
      "  4.1130660e-03 -1.9194080e-04 -4.0453614e-04  1.8324980e-03\n",
      " -1.0308829e-03 -3.6233191e-03  5.1893090e-04 -8.7444892e-04\n",
      "  7.7632815e-04  3.2474750e-03  6.4170832e-04  1.2273991e-03\n",
      " -1.9823897e-03 -4.8435866e-03  4.0076161e-04  1.7368657e-03\n",
      "  3.9631324e-03  4.5623500e-03 -1.3647181e-03 -7.3434610e-04\n",
      "  4.5945155e-03 -1.5784519e-03  2.7228801e-03  1.0700575e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define training data\n",
    "sents = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "         ['this', 'is', 'the', 'second', 'sentence'], \n",
    "         ['yet', 'another', 'sentence'],\n",
    "         ['one', 'more', 'sentence'],\n",
    "         ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# Train Model. Model expects tokenized representation of sentences \n",
    "model = Word2Vec(sents, min_count = 1)\n",
    "\n",
    "# Summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(\"vocab: \", words)\n",
    "\n",
    "# Access vector for a single word\n",
    "print(\"vector for 'sentence': \", model[\"sentence\"])\n",
    "\n",
    "# save model\n",
    "# model.save('../data/model.bin')\n",
    "\n",
    "#load model\n",
    "#m = Word2Vec.load('../data/model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Word Embeddings\n",
    "\n",
    "You can use classical projection methods to reduce the high-dimensional word vectors to two- dimensional plots and plot them on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a122f9f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "pyplot.scatter(result[:,0], result[:,1])\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i,0], result[i,1]))\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Google's Word2Vec embedding\n",
    "\n",
    "The pre-trained Google Word2Vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabyte file. \n",
    "\n",
    "You can download it from here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes. The Gensim library provides tools to load this file. Specifically, you can call the KeyedVectors.load word2vec format() function to load this model into memory, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fn = \"../data/word_vectors/GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(fn, binary=True)\n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stanford's GloVe Embedding\n",
    "\n",
    "You can download the GloVe pre-trained word vectors and load them easily with Gensim. The first step is to convert the GloVe file format to the Word2Vec file format. The only difference is the addition of a small header line. This can be done by calling the glove2word2vec() function. \n",
    "\n",
    "\n",
    "Once converted, the file can be loaded just like Word2Vec file above. Letâ€™s make this concrete with an example. You can download the smallest GloVe pre-trained model from the GloVe website. It an 822 Megabyte zip file with 4 different models (50, 100, 200 and 300-dimensional vectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary. \n",
    "\n",
    "The direct download link is here: http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "gf = \"../data/word_vectors/glove.6B.100d.txt\"\n",
    "w2v_out = \"../data/word_vectors/glove.6B.100d.txt.word2vec\"\n",
    "\n",
    "glove2word2vec(gf, w2v_out)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(w2v_out, binary=False) \n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn and Load Word Embeddings in Keras\n",
    "\n",
    "### Keras Embedding Layer\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "    - It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "    - It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "    - It can be used to load a pre-trained word embedding model, a type of transfer learning. \n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "    - input dim: This is the size of the vocabulary in the text data.\n",
    "    For example, if your data is integer encoded to values between 0-\n",
    "    10, then the size of the vocabulary would be 11 words.\n",
    "    - output dim: This is the size of the vector space in which words \n",
    "    will be embedded. It defines the size of the output vectors from \n",
    "    this layer for each word. For example, it could be 32 or 100 or \n",
    "    even larger. Test different values for your problem.\n",
    "    - input length: This is the length of input sequences, as you \n",
    "    would define for any input layer of a Keras model. For example, if \n",
    "    all of your input documents are comprised of 1000 words, this \n",
    "    would be 1000.\n",
    "    \n",
    "The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer. The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document). \n",
    "\n",
    "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Classify input as positive or negative\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "docs = ['Well done!', 'Good work', 'Great effort', \n",
    "        'nice work', 'Excellent!', 'Weak',\n",
    "        'Poor effort!', 'not good', 'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# Define class labels. 1 = positive, 0 = negative\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer Encode Documents\n",
    "\n",
    "This means that as input the Embedding layer will have sequences of integers. We could experiment with other more sophisticated bag of word model encoding like counts or TF-IDF. Keras provides the one hot() function that creates a hash of each word as an efficient integer encoding. We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 7], [16, 47], [22, 38], [26, 47], [2], [46], [45, 38], [17, 16], [45, 47], [29, 39, 7, 23]]\n"
     ]
    }
   ],
   "source": [
    "# Integer encode documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad input sequences\n",
    "\n",
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  7  0  0]\n",
      " [16 47  0  0]\n",
      " [22 38  0  0]\n",
      " [26 47  0  0]\n",
      " [ 2  0  0  0]\n",
      " [46  0  0  0]\n",
      " [45 38  0  0]\n",
      " [17 16  0  0]\n",
      " [45 47  0  0]\n",
      " [29 39  7 23]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post') \n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our Embedding layer as part of our neural network model. The Embedding layer has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions. The model is a simple binary classification model. Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0) \n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras w/ Pre-trained GloVe Embedding\n",
    "\n",
    "The Keras Embedding layer can also use a word embedding learned elsewhere.\n",
    "\n",
    "In this case, we need to be able to map words to integers as well as integers to words. Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts to sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word index attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "docs = ['Well done!', 'Good work', 'Great effort', \n",
    "        'nice work', 'Excellent!', 'Weak',\n",
    "        'Poor effort!', 'not good', 'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# Define class labels. 1 = positive, 0 = negative\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post') \n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open(\"../data/word_vectors/glove.6B.100d.txt\", mode='rt',\n",
    "          encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word]=coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be better to filter the embedding for the unique words in your training data. Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word index and locating the embedding weight vector from the loaded GloVe embedding. The result is a matrix of weights only for words we will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work 1\n",
      "done 2\n",
      "good 3\n",
      "effort 4\n",
      "poor 5\n",
      "well 6\n",
      "great 7\n",
      "nice 8\n",
      "excellent 9\n",
      "weak 10\n",
      "not 11\n",
      "could 12\n",
      "have 13\n",
      "better 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    print(word,i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False) \n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
