{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence. Neural network models are a preferred method for developing statistical language models because they can use a distributed representation where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions. In this tutorial, you will discover how to develop a statistical language model using deep learning in Python.\n",
    "\n",
    "\n",
    "## Data Prepartion \n",
    "\n",
    "### Data Overview: The Republic by Plato\n",
    "\n",
    "The Republic is the classical Greek philosopher Plato’s most famous work. It is structured as a dialog (e.g. conversation) on the topic of order and justice within a city state The entire text is available for free in the public domain. \n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "What do you see that we will need to handle in preparing the data? Here’s what I see from a quick look:\n",
    "\n",
    "    􏰀 Book/Chapter headings (e.g. BOOK I.).\n",
    "    􏰀 Lots of punctuation (e.g. -, ;-, ?-, and more).\n",
    "    􏰀 Strange names (e.g. Polemarchus).\n",
    "    􏰀 Some long monologues that go on for hundreds of lines. 􏰀 Some quoted dialog (e.g. ‘...’).\n",
    "These observations, and more, suggest at ways that we may wish to prepare the text data. The specific way we prepare the data really depends on how we intend to model it, which in turn depends on how we intend to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Design\n",
    "\n",
    "In this tutorial, we will develop a model of the text that we can then use to generate new sequences of text. The language model will be statistical and will predict the probability of each word given an input sequence of text. The predicted word will be fed in as input to in turn generate the next word. A key design decision is how long the input sequences should be. They need to be long enough to allow the model to learn the context for the words to predict. This input length will also define the length of seed text used to generate new sequences when we use the model.\n",
    "\n",
    "There is no correct answer. With enough time and resources, we could explore the ability of the model to learn with differently sized input sequences. Instead, we will pick a length of 50 words for the length of the input sequences, somewhat arbitrarily. We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "\n",
    "Instead, to keep the example brief, we will let all of the text flow together and train the model to predict the next word across sentences, paragraphs, and even books or chapters in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what\n"
     ]
    }
   ],
   "source": [
    "# Data preparation \n",
    "\n",
    "# 1. Read in file\n",
    "with open(\"../data/republic_clean.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "We need to transform the raw text into a sequence of tokens or words that we can use as a source to train the model. Based on reviewing the raw text (above), below are some specific operations we will perform to clean the text.\n",
    "\n",
    "    􏰀 Replace ‘-’ with a white space so we can split words better.\n",
    "    􏰀 Split words based on white space.\n",
    "    􏰀 Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
    "    􏰀 Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
    "    􏰀 Normalize all words to lowercase to reduce the vocabulary size.\n",
    "\n",
    "Vocabulary size is a big deal with language modeling. A smaller vocabulary results in a smaller model that trains faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
      "Total Tokens: 118684\n",
      "Unique Tokens: 7409\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import string\n",
    "\n",
    "# turn doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # strip punctuations\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "tokens = clean_doc(text)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save clean text as a set of 50 word sequences\n",
    "\n",
    "We can organize the long list of tokens into sequences of 50 input words and 1 output word. That is, sequences of 51 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 118633\n",
      "Sequence example:\n",
      " ['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was']\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "    \n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print('Sequence example:\\n', sequences[:1])\n",
    "\n",
    "# Save file \n",
    "with open(\"../data/republic_sequences.txt\", \"w\") as file:\n",
    "    data = '\\n'.join(sequences)\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Language Model\n",
    "\n",
    "We can now train a statistical language model from the prepared data. The model we will train is a neural language model. It has a few unique characteristics:\n",
    "\n",
    "    􏰀 It uses a distributed representation for words so that different words with similar meanings will have a \n",
    "      similar representation.\n",
    "    􏰀 It learns the representation at the same time as learning the model.\n",
    "    􏰀 It learns to predict the probability for the next word using the context of the last 100 words.\n",
    "\n",
    "Specifically, we will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context. Let’s start by loading our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n"
     ]
    }
   ],
   "source": [
    "# Load sequences\n",
    "with open(\"../data/republic_sequences.txt\", \"r\") as file:\n",
    "    data_seq = file.read().split('\\n')\n",
    "\n",
    "print(data_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Sequences\n",
    "\n",
    "The word embedding layer expects input sequences to be comprised of integers. We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping. To do this encoding, we will use the Tokenizer class in the Keras API.\n",
    "\n",
    "Words are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length. Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the actual vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences:  118633\n",
      "Vocab size:  7410\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Tokenize sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_seq)\n",
    "sequences = tokenizer.texts_to_sequences(data_seq)\n",
    "\n",
    "# vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Total number of sequences: \", len(sequences))\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (118633, 50)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seprated into X and y\n",
    "sequences = np.array(sequences)\n",
    "X,y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "# One hot encode y based on size of vocab\n",
    "# We want the probability distribution over the entire vocab space\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Finally, we need to specify to the Embedding layer how long input sequences are. \n",
    "# We know that there are 50 words because we designed the model, but a good generic way \n",
    "# to specify that is to use the second dimension (number of columns) of the input data’s shape. \n",
    "# That way, if you change the length of sequences when preparing data, you do not need to change \n",
    "# this data loading code; it is generic.\"\"\"\n",
    "\n",
    "print(\"X shape\", X.shape)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Fit Model\n",
    "\n",
    "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
    "\n",
    "Common values are 50, 100, and 300. We will use 50 here, but consider testing smaller or larger values. We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
    "\n",
    "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "def define_model(vocab_size, seq_length):\n",
    "    model = Sequential()\n",
    "    model.add( Embedding(vocab_size, 50, input_length=seq_length) )\n",
    "    model.add( LSTM(100, return_sequences=True) )\n",
    "    model.add( LSTM(100) )\n",
    "    model.add( Dense(100, activation='relu') )\n",
    "    model.add( Dense(vocab_size, activation='softmax') )\n",
    "    \n",
    "    # Complile \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, seq_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "118633/118633 [==============================] - 199s 2ms/step - loss: 6.1872 - acc: 0.0665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182d3f1ef0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multiclass classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model. Finally, the model is fit on the data for 100 training epochs with a modest batch size of 128 to speed things up. Training may take a few hours on modern hardware without GPUs. You can speed it up with a larger batch size and/or fewer training epochs.\n",
    "\n",
    "During training, you will see a summary of performance, including the loss and accuracy evaluated from the training data at the end of each batch update. You will get different results, but perhaps an accuracy of just over 50% of predicting the next word in the sequence, which is not bad. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but rather a model that captures the essence of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "# save the model to file\n",
    "model.save('../models/model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('../models/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Language Model\n",
    "\n",
    "We need the text so that we can choose a source sequence as input to the model for generating a new sequence of text. The model will require 50 words as input. Later, we will need to specify the expected length of input. We can determine this from the input sequences by calculating the length of one line of the loaded data and subtracting 1 for the expected output word that is also on the same line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
      "Seq length:  50\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load sequences for sample seed text\n",
    "with open(\"../data/republic_sequences.txt\", \"r\") as file:\n",
    "    data_seq = file.read().split('\\n')\n",
    "\n",
    "    print(data_seq[0])\n",
    "seq_length = len(data_seq[0].split()) - 1\n",
    "print(\"Seq length: \", seq_length)\n",
    "\n",
    "# Load model\n",
    "model = load_model('../models/model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('../models/tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man will be most likely to care about that which he loves to be sure and he will be most likely to love that which he regards as having the same interests with himself and that of which the good or evil fortune is supposed by him at any time most\n",
      "\n",
      "Predicted next word:  the\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# Seed text\n",
    "seed_text = data_seq[randint(0,len(data_seq))] \n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# Encode it\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\n",
    "# predict next word\n",
    "pred = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == pred:\n",
    "        out_word = word\n",
    "        break\n",
    "print(\"Predicted next word: \", out_word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "             if index == yhat:\n",
    "                    out_word = word\n",
    "                    break       \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word) \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Generate next 10 words\n",
    "new_seq = generate_seq( model, tokenizer, seq_length, seed_text, 10 )\n",
    "print(new_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note model was only trained for for 1 epoch for demonstration purposes. Retrain model with larger epoch size for better results. This model may take several hours to train on a gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
