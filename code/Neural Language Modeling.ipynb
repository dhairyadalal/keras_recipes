{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The neural network approach to language modeling can be described using the three following model properties, taken from A Neural Probabilistic Language Model, 2003.\n",
    "\n",
    "1. Associate each word in the vocabulary with a distributed word feature vector.\n",
    "2. Express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence.\n",
    "3. Learn simultaneously the word feature vector and the parameters of the probability function.\n",
    "\n",
    "This represents a relatively simple model where both the representation and probabilistic model are learned together directly from raw text data. Recently, the neural based approaches have started to outperform the classical statistical approaches.\n",
    "\n",
    "    We provide ample empirical evidence to suggest that connectionist \n",
    "    language mod- els are superior to standard n-gram techniques, \n",
    "    except their high computational(training) complexity.\n",
    "                — Recurrent neural network based language model, 2010.\n",
    "\n",
    "\n",
    "Initially, feedforward neural network models were used to introduce the approach. More recently, recurrent neural networks and then networks with a long-term memory like the Long Short-Term Memory network, or LSTM, allow the models to learn the relevant context over much longer input sequences than the simpler feedforward networks.\n",
    "\n",
    "    [an RNN language model] provides further generalization: instead \n",
    "    of considering just several preceding words, neurons with input \n",
    "    from recurrent connections are assumed to represent short term memory. The model learns itself from the data \n",
    "    how to represent memory. While shallow feedforward neural networks (those with just one hidden layer) can only \n",
    "    cluster similar words, recurrent neural network (which can be considered as a deep architecture) can perform \n",
    "    clustering of similar histories. This allows for instance efficient representation of patterns with variable \n",
    "    length.     — Extensions of recurrent neural network language model, 2011.\n",
    "    \n",
    "    \n",
    "Ssome heuristics for developing high-performing neural language models\n",
    "in general:\n",
    "   - **Size matters:**     \n",
    "       The best models were the largest models, specifically number of memory units.\n",
    "   \n",
    "   \n",
    "   - **Regularization matters:**  \n",
    "       Use of regularization like dropout on input connections improves results.\n",
    "   \n",
    "\n",
    "   - **CNNs vs Embeddings:** \n",
    "   \n",
    "       Character-level Convolutional Neural Network (CNN) models can be used on the front-end instead of word embeddings, achieving similar and sometimes better results.\n",
    "       \n",
    "  \n",
    "  - **Ensembles matter:**    \n",
    "       Combining the prediction from multiple models can offer large improvements in model \n",
    "     performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-based Neural Language Models\n",
    "\n",
    "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence. It is also possible to develop language models at the character level using neural networks. The benefit of character-based language models is their small vocabulary and flexibility in handling any words, punctuation, and other document structure. This comes at the cost of requiring larger models that are slower to train. Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling.\n",
    "\n",
    "### Language Model Design\n",
    "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters. The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character. After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
    "\n",
    "Longer sequences offer more context for the model to learn what character to output next but take longer to train and impose more burden on seeding the model when generating text. We will use an arbitrary length of 10 characters for this model. There is not a lot of text, and 10 characters is a few words. We can now transform the raw text into a form that our model can learn; specifically, input and output sequences of characters.\n",
    "\n",
    "### Data Preperation\n",
    "\n",
    "    1. Tokenize and clean text\n",
    "    2. Create Sequences\n",
    "    3. Encdoe Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# Load doc\n",
    "def load_doc(fn):\n",
    "    with open(fn, 'r') as file:\n",
    "        text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def generate_sequences(txt, length):\n",
    "    txt = ' '.join(txt.split()) # Strip new line chars\n",
    "    return [ txt[i-length: i+1] for i in range(len(txt)) if txt[i-length: i+1] != ''  ]\n",
    "\n",
    "def encode_seq(txt):\n",
    "    chars = sorted(list(set(txt)))\n",
    "    mapping = dict((c,i) for i,c in enumerate(chars))\n",
    "    return len(mapping), [ mapping[char] for line in txt for char in line ]\n",
    "    \n",
    "txt = load_doc(\"../data/rhyme.txt\")\n",
    "len(generate_sequences(txt, 10))\n",
    "\n",
    "vocab_size, es = encode_seq(txt)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
