{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The Keras deep learning library provides some basic tools to help you prepare your text data. The Tokenizer API that can be fit on training data and used to encode training, validation, and test documents.\n",
    "\n",
    "## Split Words with text_to_word sequence\n",
    "\n",
    "The text to word sequence() automatically does 3 things:\n",
    "    - Splits words by space\n",
    "    - Filters out punctuation\n",
    "    - Converts text to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding with one_hot vectors\n",
    "\n",
    "Keras provides the one hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one hot encoding of the document, which is not the case. Instead, the function is a wrapper for the hashing trick(). The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values. As with the text to word sequence() function in the previous section, the one hot() function will make the text lower case, filter out punctuation, and split words based on white space.\n",
    "\n",
    "** In addition to the text, the vocabulary size (total words) must be specified. The size can be larger than the existing vocabulary if additioal documents will be processed. **  The size of the vocabulary defines the hashing space from which words are hashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[1, 7, 9, 4, 3, 2, 1, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot,text_to_word_sequence \n",
    "\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3)) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash encoding w/ hashing Trick\n",
    "\n",
    "Keras provides the hashing trick() function that tokenizes and then integer encodes the document, just like the one hot() function. It provides more flexibility, allowing you to specify the hash function as either hash (the default) or other hash functions such as the built in md5 function or your own function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick, text_to_word_sequence \n",
    "\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer API\n",
    "\n",
    "Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects. Keras provides the Tokenizer class for preparing text documents for deep learning. \n",
    "\n",
    "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been\n",
    "learned about your documents:\n",
    "    - word counts: A dictionary mapping of words and their occurrence counts when the\n",
    "      Tokenizer was fit.\n",
    "    - word docs: A dictionary mapping of words and the number of documents that reach    \n",
    "      appears in.\n",
    "    - word index: A dictionary of words and their uniquely assigned integers.\n",
    "    - document count: A dictionary mapping and the number of documents they appear in \n",
    "      calculated during the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer # define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work', 'Great effort', 'nice work', 'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('well', 1),\n",
       "             ('done', 1),\n",
       "             ('good', 1),\n",
       "             ('work', 2),\n",
       "             ('great', 1),\n",
       "             ('effort', 1),\n",
       "             ('nice', 1),\n",
       "             ('excellent', 1)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "{'well': 1, 'done': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1}\n"
     ]
    }
   ],
   "source": [
    "# summarize what was learned\n",
    "display(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets. The texts to matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function. The modes available include:\n",
    "    - binary: Whether or not each word is present in the document. This is the default.\n",
    "    - count: The count of each word in the document.\n",
    "    - tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
    "    - freq: The frequency of each word as a ratio of words within each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count') \n",
    "display(encoded_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
