{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model Life-Cycle\n",
    "\n",
    "1. Define Network. -> 2. Compile Network. -> 3. Fit Network. -> 4. Evaluate Network. -> 5. Make Predictions.\n",
    "\n",
    "## Example Sequential Network\n",
    "\n",
    "Network Topology:\n",
    "\n",
    "    A multilayer perceptro that takes in 2 inputs, has 5 hidden nodes and 1 ouput layer. \n",
    "    2 -> [5] -> 1\n",
    "\n",
    "The first layer defines number of attributes. The activation function of the last layer will define the model's ouput format. \n",
    "\n",
    "** Output Layer Activation Examples **:\n",
    "\n",
    "    - Regression: Linear activation function, or linear, and the number of neurons matching the number of outputs.\n",
    "    - Binary Classification (2 class): Logistic activation function, or sigmoid, and one neuron the output layer.\n",
    "    - Multiclass Classification (>2 class): Softmax activation function, or softmax, and one output neuron per class value, assuming a one hot encoded output pattern.\n",
    "\n",
    "** Model Compilation **\n",
    "\n",
    "Compilation is an efficiency step. It transforms the simple sequence of layers that we defined into a highly efficient series of matrix transforms in a format intended to be executed on your GPU or CPU, depending on how Keras is configured. Compilation requires the optimization algorithm to use to train the network and the loss function used to evaluate the network that is minimized by the optimization algorithm. \n",
    "\n",
    "** Example Loss functions and optimizations **\n",
    "\n",
    "    Loss Fuctions\n",
    "    - Regression: Mean Squared Error or mean squared error.\n",
    "    - Binary Classification (2 class): Logarithmic Loss, also called cross entropy or binary crossentropy.\n",
    "    - Multiclass Classification (>2 class): Multiclass Logarithmic Loss or categorical crossentropy.\n",
    "    \n",
    "    Optimization Functions\n",
    "    - Stochastic Gradient Descent, or sgd, that requires the tuning of a learning rate and momentum.\n",
    "    - Adam, or adam, that requires the tuning of learning rate.\n",
    "    - RMSprop, or rmsprop, that requires the tuning of learning rate.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example Network\n",
    "# 1. Define Network\n",
    "    model = Sequential()               \n",
    "    model.add(Dense(5, input_dim=2, activation = 'relu'))\n",
    "    model.add(Dense(1, activation ='sigmoid'))\n",
    "\n",
    "# 2. Compile Network \n",
    "    model.compile(optimizer = 'sgd', loss=\"mean_squared_error\", metrics = [\"accuracy\"])\n",
    "    \n",
    "# 3. Fit Network\n",
    "    model.fit( X, y, batch_size=10, epochs=100)\n",
    "\n",
    "# 4. Evaluate Network\n",
    "      loss, accuracy = model.evaluate(X, y)\n",
    "\n",
    "# 5. Make Predictions\n",
    "      predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Functional Models\n",
    "The sequential API allows you to create models layer-by-layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs. The functional API in Keras is an alternate way of creating models that offers a lot more flexibility, including creating more complex models. It specifically allows you to define multiple input or output models as well as models that share layers. \n",
    "\n",
    "### Defining Input\n",
    "The input layer takes a shape argument that is a tuple that indicates the dimensionality of the input data. When input data is one-dimensional, such as for a Multilayer Perceptron, the shape must explicitly leave room for the shape of the mini-batch size used when splitting the data when training the network. Therefore, the shape tuple is always defined with a hanging last dimension (2,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "visible = Input(shape=(2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Layers\n",
    "The layers in the model are connected pairwise. This is done by specifying where the input comes from when defining each new layer. A bracket or functional notation is used, such that after the layer is created, the layer from which the input to the current layer comes from is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "visible = Input(shape=(2,))\n",
    "hidden = Dense(2)(visible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating The Model \n",
    "\n",
    "Keras provides a Model class that you can use to create a model from your created layers. It requires that you only specify the input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "visible = Input(shape=(2,))\n",
    "hidden = Dense(2)(visible)\n",
    "model = Model(inputs=visible, outputs=hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Network Models\n",
    "\n",
    "### Multilayer Perceptron: Binary Classification\n",
    "\n",
    "Network Topology:\n",
    "\n",
    "    10 -> [ [10] -> [20] -> [10] ] -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Define Model\n",
    "visible = Input(shape=(10,))\n",
    "hidden1 = Dense(10, activation='relu')(visible) \n",
    "hidden2 = Dense(20, activation='relu')(hidden1) \n",
    "hidden3 = Dense(10, activation='relu')(hidden2) \n",
    "output = Dense(1, activation='sigmoid')(hidden3) \n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "The model receives black and white 64 x 64 images as input, then has a sequence of two convolutional and pooling layers as feature extractors, followed by a fully connected layer to interpret the features and an output layer with a sigmoid activation for two-class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 16)        8208      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 13, 13, 10)        170       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 13, 13, 1)         11        \n",
      "=================================================================\n",
      "Total params: 8,933\n",
      "Trainable params: 8,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# Define Model\n",
    "visible = Input(shape=(64,64,1))\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1) \n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "hidden1 = Dense(10, activation='relu')(pool2) \n",
    "output = Dense(1, activation='sigmoid')(hidden1) \n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# Summarize Layers \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)\n",
    "\n",
    "The model expects 100 time steps of one feature as input. The model has a single LSTM hidden layer to extract features from the sequence, followed by a fully connected layer to interpret the LSTM output, followed by an output layer for making binary predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# Define Model\n",
    "visible = Input( shape=(100,1) )\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation = 'relu')(hidden1)\n",
    "output  = Dense(1, activation = 'sigmoid')(hidden2)\n",
    "model   = Model(inputs = visible, outputs = output)\n",
    "\n",
    "# Summarize Layers\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
