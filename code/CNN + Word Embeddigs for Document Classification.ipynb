{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification w/ Wording Embeddings and CNN\n",
    "\n",
    "Convolutional neural networks are effective at document classification, namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences.\n",
    "\n",
    "The architecture consists of three key pieces:\n",
    "    1. Word Embeddings\n",
    "    2. Convolution Model\n",
    "        - A feature extraction model that learns to extract salient \n",
    "          features from documents represented using a word embedding.\n",
    "    3. Fully Connected Model\n",
    "        - The interpretation of extracted features in terms of a \n",
    "          predictive output.\n",
    "\n",
    "Yoav Goldberg highlights the CNNs role as a feature extractor model in his book:\n",
    "\n",
    "    ... the CNN is in essence a feature-extracting architecture. It \n",
    "    does not constitute a standalone, useful network on its own, but \n",
    "    rather is meant to be integrated into a larger network, and to be \n",
    "    trained to work in tandem with it in order to produce an end \n",
    "    result. The CNNs layer’s responsibility is to extract meaningful \n",
    "    sub-structures that are useful for the overall prediction task at \n",
    "    hand.\n",
    "— Page 152, Neural Network Methods for Natural Language Processing, 2017.\n",
    "\n",
    "\n",
    "The architecture is based on the approach used by Ronan Collobert, et al. in their paper Natural Language Processing (almost) from Scratch, 2011. In it, they develop a single end-to-end neural network model with convolutional and pooling layers for use across a range of fundamental natural language processing problems. \n",
    "    \n",
    "    􏰀 Transfer function: rectified linear. \n",
    "    􏰀 Kernel sizes: 2, 4, 5.\n",
    "    􏰀 Number of filters: 100.\n",
    "    􏰀 Dropout rate: 0.5.\n",
    "    􏰀 Weight regularization (L2): 3. \n",
    "    􏰀 Batch Size: 50.\n",
    "    􏰀 Update Rule: Adadelta.\n",
    "These configurations could be used to inspire a starting point for your own experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dial in CNN hyperparameters \n",
    "\n",
    "Some hyperparameters matter more than others when tuning a convolutional neural network on your document classification problem. Ye Zhang and Byron Wallace performed a sensitivity analysis into the hyperparameters needed to configure a single layer convolutional neural network for document classification. The study is motivated by their claim that the models are sensitive to their configuration.\n",
    "\n",
    "The study makes a number of useful findings that could be used as a starting point for configuring shallow CNN models for text classification. The general findings were as follows:\n",
    "\n",
    "    􏰀 The choice of pre-trained Word2Vec and GloVe embeddings differ \n",
    "      from problem to problem, and both performed better than using \n",
    "      one hot encoded word vectors.\n",
    "    􏰀 The size of the kernel is important and should be tuned for each \n",
    "      problem.\n",
    "    􏰀 The number of feature maps is also important and should be \n",
    "      tuned.\n",
    "    􏰀 The 1-max pooling generally outperformed other types of pooling.\n",
    "    􏰀 Dropout has little effect on the model performance.\n",
    "\n",
    "They go on to provide more specific heuristics, as follows:\n",
    "\n",
    "    􏰀 Use Word2Vec or GloVe word embeddings as a starting point and \n",
    "      tune them while fitting the model.\n",
    "    􏰀 Grid search across different kernel sizes to find the optimal \n",
    "      configuration for your problem, in the range 1-10.\n",
    "    􏰀 Search the number of filters from 100-600 and explore a \n",
    "      dropout of 0.0-0.5 as part of the same search.\n",
    "    􏰀 Explore using tanh, relu, and linear activation functions.\n",
    "\n",
    "The key caveat is that the findings are based on empirical results on binary text classification problems using single sentences as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level CNNs\n",
    "\n",
    "Text documents can be modeled at the character level using convolutional neural networks that are capable of learning the relevant hierarchical structure of words, sentences, paragraphs, and more.The promise of the approach is that all of the labor-intensive effort required to clean and prepare text could be overcome if a CNN can learn to abstract the salient details.\n",
    "\n",
    "The model reads in one hot encoded characters in a fixed-sized alphabet. Encoded characters are read in blocks or sequences of 1,024 characters. A stack of 6 convolutional layers with pooling follows, with 3 fully connected layers at the output end of the network in order to make a prediction. The model achieves some success, performing better on problems that offer a larger corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper CNNs for Classification\n",
    "\n",
    "Better performance can be achieved with very deep convolutional neural networks, although standard and reusable architectures have not been adopted for classification tasks, yet. Alexis Conneau, et al. comment on the relatively shallow networks used for natural language processing and the success of much deeper networks used for computer vision applications.\n",
    "\n",
    "Key to their approach is an embedding of individual characters, rather than a word embedding:\n",
    "    \n",
    "    We present a new architecture (VDCNN) for text processing which \n",
    "    operates directly at the character level and uses only small \n",
    "    convolutions and pooling operations. \n",
    "        — Very Deep Convolutional Networks for Text Classification, \n",
    "          2016.\n",
    "\n",
    "Results on a suite of 8 large text classification tasks show better performance than more shallow networks. Specifically, state-of-the-art results on all but two of the datasets tested, at the time of writing. \n",
    "\n",
    "Generally, they make some key findings from exploring the deeper architectural approach:\n",
    "\n",
    "    􏰀 The very deep architecture worked well on small and large \n",
    "      datasets.\n",
    "    􏰀 Deeper networks decrease classification error.\n",
    "    􏰀 Max-pooling achieves better results than other, more \n",
    "      sophisticated types of pooling.\n",
    "    􏰀 Generally going deeper degrades accuracy; the shortcut \n",
    "      connections used in the architecture are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + Embedding Model for Sentiment Analysis\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "We are pretending that we are developing a system that can predict the sentiment of a textual movie review as either positive or negative. This means that after the model is developed, we will need to make predictions on new textual reviews. This will require all of the same data preparation to be performed on those new reviews as is performed on the training data for the model. \n",
    "\n",
    "### Clean and Tokenize Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # Remove punctuation \n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation)) # regex to remove punctuation\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    \n",
    "    # Remove non-alphabet chars, stop-words, and 1 letter words\n",
    "    tokens = [word for word in tokens if word.isalpha()] # remove non alphabet chars\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# filename = '../data/txt_sentoken/pos/cv000_29590.txt' \n",
    "# text = load_doc(filename)\n",
    "# tokens = clean_doc(text)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vocabulary\n",
    "\n",
    "It is important to define a vocabulary of known words when using a text model. The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. \n",
    "\n",
    "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query. Each document can be added to the counter (a new function called add doc to vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process docs())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/txt_sentoken/pos\n",
      "../data/txt_sentoken/neg\n",
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288)]\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def process_docs(directory, vocab):\n",
    "    print(directory)\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        path = directory + '/' + filename\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "# Save vocab to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "process_docs(\"../data/txt_sentoken/pos\", vocab)\n",
    "process_docs(\"../data/txt_sentoken/neg\", vocab)\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(25))\n",
    "\n",
    "# Prune vocab\n",
    "tokens = [k for k,c in vocab.items() if c >= 2]\n",
    "print(len(tokens))\n",
    "\n",
    "save_list(tokens, \"../data/txt_sentoken/vocab.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN w/ Embedding Layer\n",
    "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer. The first step is to load the vocabulary. We will use it to filter out words from movie reviews that we are not interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Load vocab into memory\n",
    "def load_vocab(filename):\n",
    "    file = open(filename, 'r')\n",
    "    txt = file.read()\n",
    "    file.close()\n",
    "    return txt\n",
    "\n",
    "# tokenize doc and filter out words not in vocab\n",
    "def cnn_clean_doc(doc, vocab):\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation)) # regex to remove punctuation\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    \n",
    "    tokens = ' '.join([w for w in tokens if w in vocab])\n",
    "    return tokens\n",
    "\n",
    "# Load all docs in a directory\n",
    "def cnn_process_docs(dir_, vocab, is_train):\n",
    "    docs = list()\n",
    "    for fn in listdir(dir_):\n",
    "        if is_train and fn.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not fn.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        path = dir_ + '/' + fn\n",
    "        doc = load_doc(path)\n",
    "        tokens = cnn_clean_doc(doc, vocab)\n",
    "        docs.append(tokens)\n",
    "    return docs\n",
    "\n",
    "# Load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    neg = cnn_process_docs('../data/txt_sentoken/neg', vocab, is_train)\n",
    "    pos = cnn_process_docs(\"../data/txt_sentoken/pos\", vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    labels = np.array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding Layer Preparation\n",
    "The next step is to encode each document as a sequence of integers. The Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific real-valued vector representation within the embedding. These vectors are random at the beginning of training, but during training become meaningful to the network. We can encode the training documents as sequences of integers using the Tokenizer class in the Keras API. First, we must construct an instance of the class then train it on all documents in the training dataset. In this case, it develops a vocabulary of all tokens in the training dataset and develops a consistent mapping from words in the vocabulary to unique integers. We could just as easily develop this mapping ourselves using our vocabulary file. The create tokenizer() function below will prepare a Tokenizer from the training data.\n",
    "\n",
    "#### Text Padding\n",
    "We also need to ensure that all documents have the same length. We could truncate reviews to the smallest size or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. In this case, we will pad all reviews to the length of the longest review in the training dataset. First, we can find the longest review using the max() function on the training dataset and take its length. We can then call the Keras function pad sequences() to pad the sequences to the maximum length by adding 0 values on the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    return pad_sequences(encoded, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "We are now ready to define our neural network model. The model will use an Embedding layer as the first hidden layer. The Embedding layer requires the specification of the vocabulary size, the size of the real-valued vector space, and the maximum length of input documents. The vocabulary size is the total number of words in our vocabulary, plus one for unknown words\n",
    "\n",
    "We will use a 100-dimensional vector space. The maximum document length was calculated above in the max length variable used during padding. \n",
    "\n",
    "Model architecture:\n",
    "    - Embedding Layer\n",
    "    - CNN layer\n",
    "        - 32 filters (parallel fields for processing words), kernel size of 8 with a relu activation function. \n",
    "    - Pooling layer that reduces the output of the convolutional layer by half\n",
    "    - 2D output from the CNN part of the model is flattened to one long 2D vector to represent the features  \n",
    "      extracted by the CNN. \n",
    "    - Output layer w/ sigmoid activation function (0 for negative review and 1 for positive review) \n",
    "\n",
    "The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "# define model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add( Embedding(vocab_size, 100, input_length = max_length) )\n",
    "    model.add( Conv1D(filters = 32, kernel_size = 8, activation = 'relu') )\n",
    "    model.add( MaxPooling1D(pool_size = 2) )\n",
    "    model.add( Flatten() )\n",
    "    model.add( Dense(10, activation = 'relu') )\n",
    "    model.add( Dense(1, activation = 'sigmoid') )\n",
    "    \n",
    "    # compile network \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  25768\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model ready to fit\n"
     ]
    }
   ],
   "source": [
    "# Load data and run model\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# Load vocab into memory\n",
    "vocab = load_vocab(\"../data/txt_sentoken/vocab.txt\")\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# Load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# Define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "# Encode Training data\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "\n",
    "# Define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "model.summary()\n",
    "print(\"Model ready to fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 10s 6ms/step - loss: 0.6904 - acc: 0.5339\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 10s 6ms/step - loss: 0.4605 - acc: 0.8344\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 10s 6ms/step - loss: 0.0432 - acc: 0.9917\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 10s 6ms/step - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 0.0014 - acc: 0.9994\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 9.3582e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 6.8913e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 5.4735e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 3.8351e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=10, verbose=1)\n",
    "\n",
    "# save the model\n",
    "model.save('../data/txt_sentoken/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317\n",
      "1317\n",
      "Model loaded. ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Load Train and Test docs\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)  # Tokenizer\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "# Encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "\n",
    "# Load model\n",
    "#model = load_model(\"../data/txt_sentoken/model.h5\")\n",
    "print(\"Model loaded. ready for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.000000\n",
      "Test Accuracy: 85.500000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on training set\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose = 0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0) \n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop predictor for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317\n",
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: NEGATIVE (57.440%)\n",
      "1317\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (63.011%)\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    line = cnn_clean_doc(review, vocab)\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    yhat = model.predict(padded, verbose = 0 )\n",
    "    \n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE' \n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram CNN Model for Sentiment Analysis\n",
    "\n",
    "A standard deep learning model for text classification and sentiment analysis uses a word embedding layer and one-dimensional convolutional neural network. The model can be expanded by using multiple parallel convolutional neural networks that read the source document using different kernel sizes. This, in effect, creates a multichannel convolutional neural network for text that reads text with different n-gram sizes (groups of words).\n",
    "\n",
    "### Model\n",
    "\n",
    "A multi-channel convolutional neural network for document classification involves using multiple versions of the standard model with different sized kernels. This allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations. This approach was first described by Yoon Kim in his 2014 paper titled Convolutional Neural Networks for Sentence Classification. \n",
    "\n",
    "In Keras, a multiple-input model can be defined using the functional API. We will define a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text. Each channel is comprised of the following elements:\n",
    "    \n",
    "    - Input layer that defines the length of input sequences.\n",
    "    - Embedding layer set to the size of the vocabulary and 100-dimensional real-valued representations.\n",
    "    - Conv1D layer with 32 filters and a kernel size set to the number of words to read at once.\n",
    "    - MaxPooling1D layer to consolidate the output from the convolutional layer.\n",
    "    - Flatten layer to reduce the three-dimensional output to two dimensional for concatenation.\n",
    "\n",
    "The output from the three channels are concatenated into a single vector and process by a Dense layer and an output layer. The function below defines and returns the model. As part of defining the model, a summary of the defined model is printed and a plot of the model graph is created and saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 100)      1000        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 10, 100)      1000        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 10, 100)      1000        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 7, 32)        12832       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 5, 32)        19232       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3, 32)        25632       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 7, 32)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5, 32)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 3, 32)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 3, 32)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2, 32)        0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 32)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1930        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 62,637\n",
      "Trainable params: 62,637\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D \n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def define_ngram_model(length, vocab_size):\n",
    "    \n",
    "    # Channel 1\n",
    "    in1 = Input(shape=(length,))\n",
    "    eb1 = Embedding(vocab_size, 100)(in1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(eb1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # Channel 2\n",
    "    in2 = Input(shape=(length,))\n",
    "    eb2 = Embedding(vocab_size, 100)(in2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(eb2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    \n",
    "    # Channel 3\n",
    "    in3 = Input(shape=(length,))\n",
    "    eb3 = Embedding(vocab_size, 100)(in3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(eb3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # Merge channels\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[in1, in2, in3], outputs=outputs)\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # summarize\n",
    "    return model\n",
    "\n",
    "define_ngram_model(10,10).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  25768\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 25768)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 25768)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 25768)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 25768, 100)   131700      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 25768, 100)   131700      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 25768, 100)   131700      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 25765, 32)    12832       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 25763, 32)    19232       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 25761, 32)    25632       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 25765, 32)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 25763, 32)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 25761, 32)    0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 12882, 32)    0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 12881, 32)    0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 12880, 32)    0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 412224)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 412192)       0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 412160)       0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1236576)      0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           12365770    concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,818,577\n",
      "Trainable params: 12,818,577\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model ready to fit\n"
     ]
    }
   ],
   "source": [
    "# Load data and run model\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "# Load vocab into memory\n",
    "vocab = load_vocab(\"../data/txt_sentoken/vocab.txt\")\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# Load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# Define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "# Encode Training data\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "\n",
    "# Define model\n",
    "model = define_ngram_model(vocab_size, max_length)\n",
    "\n",
    "model.summary()\n",
    "print(\"Model ready to fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Load Train and Test docs\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "\n",
    "tokenizer = create_tokenizer(train_docs)  # Tokenizer\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "# Encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "\n",
    "# Load model\n",
    "#model = load_model(\"../data/txt_sentoken/model.h5\")\n",
    "print(\"Model loaded. ready for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 1 arrays: [array([[   27,    27,    27, ...,     0,     0,     0],\n       [   74,  1536,  1426, ...,     0,     0,     0],\n       [ 7430,     3, 16201, ...,     0,     0,     0],\n       ...,\n       [ 1078,   27...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e92245feab7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate model on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# evaluate model on test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1769\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 1 arrays: [array([[   27,    27,    27, ...,     0,     0,     0],\n       [   74,  1536,  1426, ...,     0,     0,     0],\n       [ 7430,     3, 16201, ...,     0,     0,     0],\n       ...,\n       [ 1078,   27..."
     ]
    }
   ],
   "source": [
    "# Evaluate model on training set\n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose = 0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0) \n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
