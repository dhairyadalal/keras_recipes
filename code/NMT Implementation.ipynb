{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German to English Dataset\n",
    "\n",
    "In this tutorial, we will use a dataset of German to English terms used as the basis for flashcards for language learning. The dataset is available from the ManyThings.org website, with examples drawn from the Tatoeba Project. \n",
    "    \n",
    "    􏰀 Download the English-German pairs dataset: http://www.manythings.org/anki/deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We are now ready to clean each sentence. The specific cleaning operations we will perform are as follows:\n",
    "\n",
    "    􏰀 Remove all non-printable characters.\n",
    "    􏰀 Remove all punctuation characters.\n",
    "    􏰀 Normalize all Unicode characters to ASCII (e.g. Latin characters). \n",
    "    􏰀 Normalize the case to lowercase.\n",
    "    􏰀 Remove any remaining tokens that are not alphabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi.', 'Hallo!'], ['Hi.', 'Grüß Gott!'], ['Run!', 'Lauf!'], ['Fire!', 'Feuer!'], ['Help!', 'Hilfe!'], ['Help!', 'Zu Hülf!'], ['Stop!', 'Stopp!'], ['Wait!', 'Warte!'], ['Go on.', 'Mach weiter.'], ['Hello!', 'Hallo!']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/deu.txt\") as file:\n",
    "    doc = file.read().strip().split('\\n')\n",
    "    pairs = [ line.split('\\t') for line in doc  ]\n",
    "    file.close()\n",
    "    \n",
    "print(pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi' 'hallo']\n",
      " ['hi' 'gru gott']\n",
      " ['run' 'lauf']]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pickle \n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            line = [re_punc.sub('', w) for w in line]\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)    \n",
    "\n",
    "cleaned_pairs = clean_pairs(pairs)\n",
    "print(cleaned_pairs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving.\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset\n",
    "with open(\"../data/translation_pairs.pkl\", \"wb\") as file:\n",
    "    pickle.dump(cleaned_pairs, file)\n",
    "    file.close()\n",
    "    \n",
    "print(\"Finished saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data in Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Dataset size:  169813\n",
      "saved:  ../data/english-german-both.pkl\n",
      "saved:  ../data/english-german-train.pkl\n",
      "saved:  ../data/english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from numpy.random import shuffle\n",
    "\n",
    "#load cleaned pairs\n",
    "#cleaned_pairs = pickle.load(open(\"../data/translation_pairs.pkl\",\"rb\"))\n",
    "\n",
    "def save_data(data, filename):\n",
    "    pickle.dump( data, open(filename, \"wb\") )\n",
    "    print(\"saved: \", filename)\n",
    "    \n",
    "print(\"Raw Dataset size: \", len(cleaned_pairs))\n",
    "\n",
    "# Reduce dataset size for training demo\n",
    "n_sentences = 10000\n",
    "dataset = cleaned_pairs[:n_sentences, :]\n",
    "shuffle(dataset)\n",
    "\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "save_data(dataset, '../data/english-german-both.pkl') \n",
    "save_data(train, '../data/english-german-train.pkl') \n",
    "save_data(test, '../data/english-german-test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_data(filename):\n",
    "    return pickle.load(open(filename,'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_data('../data/english-german-both.pkl') \n",
    "train = load_data('../data/english-german-train.pkl')\n",
    "test = load_data('../data/english-german-test.pkl')\n",
    "\n",
    "# Fit tokenizer\n",
    "def create_tokenizer(data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(data):\n",
    "    return max( len(line.split()) for line in data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizers and encode sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english vocab size 2315, seq length: 5\n",
      "german vocab size 3686, seq length: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np \n",
    "\n",
    "# Encode sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# One hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "\n",
    "# English tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:,0])\n",
    "print(\"english vocab size %i, seq length: %i\" %(eng_vocab_size, eng_length))\n",
    "\n",
    "# German Tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1]) \n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1 \n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print(\"german vocab size %i, seq length: %i\" %(ger_vocab_size, ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:  (9000, 10) Y train:  (9000, 5, 2315)\n",
      "X test:  (1000, 10) Y test:  (1000, 5, 2315)\n"
     ]
    }
   ],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "print(\"X train: \", trainX.shape, \"Y train: \", trainY.shape)\n",
    "\n",
    "# Prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "print(\"X test: \", testX.shape, \"Y test: \", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 256)           943616    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 2315)           594955    \n",
      "=================================================================\n",
      "Total params: 2,589,195\n",
      "Trainable params: 2,589,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, \n",
    "                 src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, \n",
    "                        input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True)) \n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax'))) # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "    \n",
    "    \n",
    "model = define_model(ger_vocab_size, eng_vocab_size, \n",
    "                     ger_length, eng_length, 256)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 19s - loss: 4.3151 - val_loss: 3.5629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.56295, saving model to ../models/nmt_model.h5\n",
      "Epoch 2/30\n",
      " - 18s - loss: 3.4112 - val_loss: 3.4244\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.56295 to 3.42444, saving model to ../models/nmt_model.h5\n",
      "Epoch 3/30\n",
      " - 18s - loss: 3.2742 - val_loss: 3.3699\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.42444 to 3.36994, saving model to ../models/nmt_model.h5\n",
      "Epoch 4/30\n",
      " - 18s - loss: 3.1453 - val_loss: 3.2564\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.36994 to 3.25637, saving model to ../models/nmt_model.h5\n",
      "Epoch 5/30\n",
      " - 18s - loss: 3.0051 - val_loss: 3.1740\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.25637 to 3.17402, saving model to ../models/nmt_model.h5\n",
      "Epoch 6/30\n",
      " - 18s - loss: 2.8607 - val_loss: 3.0734\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.17402 to 3.07337, saving model to ../models/nmt_model.h5\n",
      "Epoch 7/30\n",
      " - 18s - loss: 2.7196 - val_loss: 2.9614\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.07337 to 2.96144, saving model to ../models/nmt_model.h5\n",
      "Epoch 8/30\n",
      " - 18s - loss: 2.5570 - val_loss: 2.8535\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.96144 to 2.85348, saving model to ../models/nmt_model.h5\n",
      "Epoch 9/30\n",
      " - 18s - loss: 2.3874 - val_loss: 2.7565\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.85348 to 2.75652, saving model to ../models/nmt_model.h5\n",
      "Epoch 10/30\n",
      " - 18s - loss: 2.2189 - val_loss: 2.6424\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.75652 to 2.64240, saving model to ../models/nmt_model.h5\n",
      "Epoch 11/30\n",
      " - 18s - loss: 2.0681 - val_loss: 2.5605\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.64240 to 2.56051, saving model to ../models/nmt_model.h5\n",
      "Epoch 12/30\n",
      " - 18s - loss: 1.9253 - val_loss: 2.5040\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.56051 to 2.50397, saving model to ../models/nmt_model.h5\n",
      "Epoch 13/30\n",
      " - 18s - loss: 1.7934 - val_loss: 2.4237\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.50397 to 2.42374, saving model to ../models/nmt_model.h5\n",
      "Epoch 14/30\n",
      " - 18s - loss: 1.6651 - val_loss: 2.3703\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.42374 to 2.37027, saving model to ../models/nmt_model.h5\n",
      "Epoch 15/30\n",
      " - 18s - loss: 1.5468 - val_loss: 2.3242\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.37027 to 2.32424, saving model to ../models/nmt_model.h5\n",
      "Epoch 16/30\n",
      " - 18s - loss: 1.4367 - val_loss: 2.2806\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.32424 to 2.28056, saving model to ../models/nmt_model.h5\n",
      "Epoch 17/30\n",
      " - 18s - loss: 1.3341 - val_loss: 2.2408\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.28056 to 2.24079, saving model to ../models/nmt_model.h5\n",
      "Epoch 18/30\n",
      " - 18s - loss: 1.2341 - val_loss: 2.2218\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.24079 to 2.22183, saving model to ../models/nmt_model.h5\n",
      "Epoch 19/30\n",
      " - 18s - loss: 1.1393 - val_loss: 2.1910\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.22183 to 2.19105, saving model to ../models/nmt_model.h5\n",
      "Epoch 20/30\n",
      " - 18s - loss: 1.0513 - val_loss: 2.1545\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.19105 to 2.15453, saving model to ../models/nmt_model.h5\n",
      "Epoch 21/30\n",
      " - 18s - loss: 0.9655 - val_loss: 2.1315\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.15453 to 2.13146, saving model to ../models/nmt_model.h5\n",
      "Epoch 22/30\n",
      " - 18s - loss: 0.8880 - val_loss: 2.1231\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.13146 to 2.12312, saving model to ../models/nmt_model.h5\n",
      "Epoch 23/30\n",
      " - 18s - loss: 0.8117 - val_loss: 2.0994\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.12312 to 2.09940, saving model to ../models/nmt_model.h5\n",
      "Epoch 24/30\n",
      " - 18s - loss: 0.7416 - val_loss: 2.0939\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.09940 to 2.09387, saving model to ../models/nmt_model.h5\n",
      "Epoch 25/30\n",
      " - 18s - loss: 0.6790 - val_loss: 2.0775\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.09387 to 2.07754, saving model to ../models/nmt_model.h5\n",
      "Epoch 26/30\n",
      " - 18s - loss: 0.6191 - val_loss: 2.0853\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.07754\n",
      "Epoch 27/30\n",
      " - 18s - loss: 0.5636 - val_loss: 2.0721\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.07754 to 2.07208, saving model to ../models/nmt_model.h5\n",
      "Epoch 28/30\n",
      " - 18s - loss: 0.5137 - val_loss: 2.0779\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.07208\n",
      "Epoch 29/30\n",
      " - 18s - loss: 0.4690 - val_loss: 2.0742\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.07208\n",
      "Epoch 30/30\n",
      " - 18s - loss: 0.4296 - val_loss: 2.0713\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.07208 to 2.07135, saving model to ../models/nmt_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6edf8f518>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('../models/nmt_model.h5', monitor='val_loss',\n",
    "                             verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, \n",
    "          validation_data=(testX, testY),\n",
    "          callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets if not loaded\n",
    "dataset = load_data('../data/english-german-both.pkl')\n",
    "train = load_data('../data/english-german-train.pkl') \n",
    "test = load_data('../data/english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import argmax\n",
    "\n",
    "# load model\n",
    "#model = load_model(\"../models/nmt_model.h5\")\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append(raw_target.split())\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[wo seid ihr], target=[where are you], predicted=[where are you]\n",
      "src=[ich bin mude], target=[im sleepy], predicted=[im tired]\n",
      "src=[ich sah nichts], target=[i saw nothing], predicted=[i didnt nothing]\n",
      "src=[ich habe kaffee getrunken], target=[i drank coffee], predicted=[i love coffee]\n",
      "src=[tom ging raus], target=[tom walked out], predicted=[tom went out]\n",
      "src=[ich bin dein chef], target=[im your boss], predicted=[im your boss]\n",
      "src=[das ist keine neuigkeit], target=[this isnt news], predicted=[this isnt news]\n",
      "src=[mach es aus], target=[turn it off], predicted=[do it off]\n",
      "src=[ich habe mir sorgen gemacht], target=[i was worried], predicted=[i been abroad]\n",
      "src=[tom ist erstaunlich], target=[toms amazing], predicted=[toms amazing]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhairya/.conda/envs/nlp/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dhairya/.conda/envs/nlp/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dhairya/.conda/envs/nlp/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.074037\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "test\n",
      "src=[ich ziehe mich aus], target=[im undressing], predicted=[i am undressing]\n",
      "src=[tom wird nicht warten], target=[tom wont wait], predicted=[tom wont talk]\n",
      "src=[tom ist pleite], target=[tom is broke], predicted=[tom is gasping]\n",
      "src=[tom hat gebetet], target=[tom prayed], predicted=[tom obeyed]\n",
      "src=[ich studiere englisch], target=[i study english], predicted=[i teach english]\n",
      "src=[tom wird arbeiten], target=[tom will work], predicted=[tom will]\n",
      "src=[fur hier oder zum mitnehmen], target=[here or to go], predicted=[here or do]\n",
      "src=[ich bin knapp bei kasse], target=[im broke], predicted=[im in free]\n",
      "src=[das ist ekelhaft], target=[this is ugly], predicted=[this is]\n",
      "src=[die schule ist vorbei], target=[schools out], predicted=[school is over]\n",
      "BLEU-1: 0.065689\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, testX, test)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python - nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
